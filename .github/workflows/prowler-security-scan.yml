name: ProwlerSecurityScan

on:
  # PR에서는 보안 점검/ 리포트 생성
  pull_request:
  # main 브랜치에 머지되면 remediation + PR 자동 생성까지 수행
  push:
    branches: ["main"]
  # 수동 재실행(Apply 이후 재스캔 트리거용)
  workflow_dispatch:

# 기본은 최소 권한 (읽기 전용)
permissions:
  contents: read

jobs:

  # ================================
  # 1) SCAN JOB
  # AWS에 실제로 접속해서 Prowler 보안 점검 수행
  # ================================
  scan:
    name: 1) Scan (Prowler CIS 1.4)
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write   # OIDC role assume

    steps:
      # 레포 코드 체크아웃
      - name: Checkout repository
        uses: actions/checkout@v4

      # 결과 저장 디렉토리 생성
      - name: Create working directories
        run: |
          mkdir -p output reports scripts mcp/output

      # Python 환경 구성
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # GitHub OIDC → AWS IAM Role Assume
      - name: Configure AWS credentials (OIDC assume-role)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ap-northeast-2

      # Prowler 및 리포트 라이브러리 설치
      - name: Install dependencies (Prowler + report tools)
        run: |
          pip install prowler pandas openpyxl boto3

      # CIS 1.4 기준으로 AWS 스캔 실행
      - name: Run Prowler scan (CIS 1.4 / ap-northeast-2)
        continue-on-error: true
        run: |
          prowler aws \
            --region ap-northeast-2 \
            --compliance cis_1.4_aws \
            --severity medium \
            --output-directory output \
            --output-formats json-ocsf html csv

      # 원본 결과 아티팩트 업로드 (다음 job에서 사용)
      - name: "Upload artifacts: Prowler raw output"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: prowler-output
          path: output/


  # ================================
  # 2) PROCESS JOB
  # Scan 결과를 정규화/점수화/AI보강/Runbook 생성
  # ================================
  process:
    name: 2) Process (Normalize/Score/AI/Runbook/OCSF)
    runs-on: ubuntu-latest
    needs: scan
    permissions:
      contents: read
      id-token: write   # Bedrock 호출 시 OIDC assume-role 가능하도록

    steps:
      # 레포 코드 체크아웃
      - name: Checkout repository
        uses: actions/checkout@v4

      # 작업 디렉토리 생성
      - name: Create working directories
        run: |
          mkdir -p output reports scripts mcp/output

      # scan job에서 업로드한 원본 결과 다운로드
      - name: "Download artifacts: Prowler raw output"
        uses: actions/download-artifact@v4
        with:
          name: prowler-output
          path: output

      # Python 환경 구성
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # GitHub OIDC → AWS IAM Role Assume (AI Assist용, fork PR에서는 건너뜀)
      - name: Configure AWS credentials (OIDC assume-role)
        if: github.event_name == 'push' || github.event.pull_request.head.repo.fork != true
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ap-northeast-2

      # 파이프라인 처리용 라이브러리 설치
      - name: Install dependencies (pipeline tools)
        run: |
          pip install pandas openpyxl boto3

      # Normalize → Score → AI Assist → Runbook → OCSF export
      - name: Process findings pipeline
        run: |
          set -e

          # CSV가 있을 경우만 처리
          if ls output/*.csv >/dev/null 2>&1; then
            python mcp/pipeline/normalize.py \
              --input "output/*.csv" \
              --output mcp/output/findings-normalized.csv

            python mcp/pipeline/score.py \
              --input mcp/output/findings-normalized.csv \
              --output mcp/output/findings-scored.csv

            python mcp/pipeline/ai_assist.py \
              --input mcp/output/findings-scored.csv \
              --output mcp/output/findings-scored-ai.csv

            python mcp/pipeline/build_runbook.py \
              --input mcp/output/findings-scored-ai.csv \
              --template mcp/templates/runbook.md \
              --output mcp/output/runbook.md
          fi

          # OCSF export 처리
          if ls output/*.ocsf.json >/dev/null 2>&1; then
            python mcp/pipeline/export_ocsf.py \
              --input "output/*.ocsf.json" \
              --output mcp/output/ocsf-findings.json
          fi

      # 가공된 결과 업로드
      - name: "Upload artifacts: MCP processed output"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mcp-output
          path: mcp/output/


  # ================================
  # 3) REMEDIATE JOB
  # main push일 때만 Terraform 코드 생성 + PR 자동 생성
  # ================================
  remediate:
    name: 3) Remediate (Generate IaC + Create PR)
    runs-on: ubuntu-latest
    needs: process
    # Gate 조건: main push일 때만 실행
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    permissions:
      contents: write
      pull-requests: write
      id-token: write

    steps:
      # 브랜치 생성/푸시를 위해 full clone
      - name: Checkout repository (full history)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # MCP 가공 결과 다운로드
      - name: "Download artifacts: MCP processed output"
        uses: actions/download-artifact@v4
        with:
          name: mcp-output
          path: mcp/output

      # Python 환경 구성.
      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # GitHub OIDC → AWS IAM Role Assume (Bedrock 호출용)
      - name: Configure AWS credentials (OIDC assume-role)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ap-northeast-2

      # IaC 보조 라이브러리 설치
      - name: Install IaC helper libraries
        run: |
          pip install pandas pyyaml boto3

      # Terraform CLI 설치 (검증용)
      - name: Setup Terraform CLI
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      # AI 기반 Terraform remediation 코드 생성
      - name: Generate Terraform remediation
        env:
          PREFER_IAC_SNIPPET: "false"
        run: |
          mkdir -p remediation
          # 이번 실행에서 생성된 파일만 유지 (이전 결과 정리)
          find remediation -type f ! -name '.gitkeep' -delete

          if [ -f mcp/output/findings-scored-ai.csv ]; then
            python mcp/pipeline/generate_remediation.py \
              --input mcp/output/findings-scored-ai.csv \
              --output-dir remediation/
          else
            echo "No scored findings found. Skipping remediation."
            exit 0
          fi

      # 생성된 .tf 파일이 있을 경우 카테고리별 PR 생성
      - name: Create remediation PRs (per category)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git remote set-url origin "https://x-access-token:${{ secrets.PROWLER_PAT }}@github.com/${{ github.repository }}.git"

          MANIFEST="remediation/manifest.json"
          if [ ! -f "$MANIFEST" ]; then
            echo "No manifest.json found; skipping PR creation."
            exit 0
          fi

          # 생성된 파일을 임시 디렉터리에 백업 (checkout 시 유실 방지)
          TMPGEN=$(mktemp -d)
          cp -r remediation/* "$TMPGEN/"
          echo "Backed up generated files to $TMPGEN"

          # manifest.json에서 카테고리 목록 추출
          CATEGORIES=$(python3 -c "
          import json, sys
          with open('$TMPGEN/manifest.json') as f:
              items = json.load(f)
          cats = sorted(set(item['category'] for item in items))
          for c in cats:
              print(c)
          ")

          if [ -z "$CATEGORIES" ]; then
            echo "No categories found in manifest."
            exit 0
          fi

          for CATEGORY in $CATEGORIES; do
            echo "=== Processing category: $CATEGORY ==="

            # 해당 카테고리 파일명 목록 추출 (basename만)
            CAT_FILES=$(python3 -c "
          import json
          with open('$TMPGEN/manifest.json') as f:
              items = json.load(f)
          for item in items:
              if item['category'] == '$CATEGORY':
                  print(item['file'])
          ")

            if [ -z "$CAT_FILES" ]; then
              echo "No files for category $CATEGORY; skipping."
              continue
            fi

            BRANCH="remediation/$CATEGORY"
            git checkout -B "$BRANCH" main

            # 기존 remediation 파일 정리 후 해당 카테고리 파일만 복사
            mkdir -p remediation
            for fname in $CAT_FILES; do
              if [ -f "$TMPGEN/$fname" ]; then
                cp "$TMPGEN/$fname" "remediation/$fname"
                git add "remediation/$fname"
              fi
            done

            # 카테고리별 manifest 생성 (머지 충돌 방지)
            python3 -c "
          import json
          with open('$TMPGEN/manifest.json') as f:
              items = json.load(f)
          filtered = [i for i in items if i['category'] == '$CATEGORY']
          with open('remediation/manifest.json', 'w') as f:
              json.dump(filtered, f, indent=2)
          "
            git add remediation/manifest.json

            if [ -z "$(git diff --cached --name-only)" ]; then
              echo "No changes for $CATEGORY; skipping."
              continue
            fi

            FILE_COUNT=$(echo "$CAT_FILES" | wc -l | tr -d ' ')
            git commit -m "fix($CATEGORY): auto-generated Terraform remediation [$FILE_COUNT files]"
            git push origin "$BRANCH" --force

            # 기존 PR이 있으면 업데이트, 없으면 생성
            EXISTING_PR=$(gh pr list --head "$BRANCH" --base main --state open --json number --jq '.[0].number' 2>/dev/null || echo "")
            if [ -z "$EXISTING_PR" ]; then
              PR_BODY=$(python3 -c "
          import json
          with open('$TMPGEN/manifest.json') as f:
              items = json.load(f)
          lines = ['## Category: \`$CATEGORY\`', '', 'Auto-generated Terraform remediation from Prowler pipeline.', '', '**Files:** $FILE_COUNT remediation(s)', '', '### Included checks:']
          for item in items:
              if item['category'] == '$CATEGORY':
                  lines.append(f\"- [{item['priority']}] {item['check_title']} (\`{item['check_id']}\`)\")
          print('\n'.join(lines))
          ")
              gh pr create \
                --title "Security Remediation: $CATEGORY ($FILE_COUNT files)" \
                --body "$PR_BODY" \
                --base main \
                --head "$BRANCH"
              echo "Created PR for category: $CATEGORY"
            else
              echo "PR #${EXISTING_PR} already exists for ${BRANCH} – updated via force-push."
            fi
          done

          rm -rf "$TMPGEN"
