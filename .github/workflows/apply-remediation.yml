name: Apply Remediation

on:
  pull_request:
    types: [opened, synchronize, closed]
    paths:
      - 'remediation/*.tf'

env:
  TF_BACKEND_BUCKET: prowler-terraform-state-132410971304
  TF_BACKEND_TABLE: prowler-terraform-locks
  TF_BACKEND_REGION: ap-northeast-2

permissions:
  contents: read
  id-token: write
  actions: write
  pull-requests: write

jobs:
  # -------------------------------------------------------
  # Plan: run on PR updates and post Terraform plan comment
  # -------------------------------------------------------
  plan:
    if: github.event.pull_request.merged != true && github.event.action != 'closed'
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ap-northeast-2

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Setup Infracost
        uses: infracost/actions/setup@v3
        with:
          api-key: ${{ secrets.INFRACOST_API_KEY }}  

      - name: Install IaC quality tools (tflint + tfsec)
        run: |
          curl -sSL -o /usr/local/bin/tfsec https://github.com/aquasecurity/tfsec/releases/latest/download/tfsec-linux-amd64
          chmod +x /usr/local/bin/tfsec
          curl -sSL -o /tmp/tflint.zip https://github.com/terraform-linters/tflint/releases/latest/download/tflint_linux_amd64.zip
          unzip -q /tmp/tflint.zip -d /tmp
          mv /tmp/tflint /usr/local/bin/tflint
          tflint --init

      - name: Collect changed files and group by category
        id: changed
        run: |
          files=$(git diff --name-status ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} -- 'remediation/*.tf' | awk '$1!="D"{print $NF}' | tr '\n' ' ')
          echo "files=$files" >> $GITHUB_OUTPUT

          # Extract unique categories from changed files
          # fix-iam_password_policy_xxx.tf -> iam
          # fix-cloudwatch_log_group_xxx.tf -> cloudwatch
          categories=""
          for file in $files; do
            base=$(basename "$file" .tf | sed 's/^fix-//')
            cat=$(echo "$base" | sed 's/_.*//')
            if ! echo "$categories" | grep -qw "$cat"; then
              categories="$categories $cat"
            fi
          done
          echo "categories=$(echo $categories | xargs)" >> $GITHUB_OUTPUT

      - name: Terraform Plan (per category)
        id: plan
        continue-on-error: true
        run: |
          set -euo pipefail
          : > /tmp/tfplan.txt
          : > /tmp/tfgate.txt
          exitcode=0

          if [ -z "${{ steps.changed.outputs.categories }}" ]; then
            echo "No remediation categories changed." >> /tmp/tfplan.txt
          else
            for category in ${{ steps.changed.outputs.categories }}; do
              echo "## Category: $category" >> /tmp/tfplan.txt
              echo "## Category: $category" >> /tmp/tfgate.txt
              work=$(mktemp -d)
              state_key="remediation/${category}.tfstate"

              # Copy only files listed in remediation/manifest.json for this category.
              # This prevents stale category files from being applied accidentally.
              file_count=0
              cat_files=$(CATEGORY="$category" python3 - <<'PY'
          import json
          import os

          category = os.environ.get("CATEGORY", "").strip()
          manifest = "remediation/manifest.json"
          selected = []
          if category and os.path.exists(manifest):
              with open(manifest, "r", encoding="utf-8") as f:
                  items = json.load(f)
              seen = set()
              for item in items:
                  if str(item.get("category", "")).strip() != category:
                      continue
                  fname = str(item.get("file", "")).strip()
                  if not fname or fname in seen:
                      continue
                  seen.add(fname)
                  selected.append(f"remediation/{fname}")
          print("\n".join(selected))
          PY
              )
              for f in $cat_files; do
                [ -f "$f" ] || continue
                cp "$f" "$work/"
                file_count=$((file_count+1))
              done

              if [ "$file_count" -eq 0 ]; then
                echo "SKIP $category: no files found" | tee -a /tmp/tfplan.txt /tmp/tfgate.txt
                exitcode=1
                continue
              fi

              echo "  Files: $file_count" >> /tmp/tfplan.txt

              # Run cleanup on each file
              for tf in "$work"/*.tf; do
                python3 iac/scripts/cleanup_hcl.py "$tf"
              done

              # Deduplicate resource/data blocks across files
              python3 iac/scripts/dedup_resources.py "$work"

              # Copy optional tfvars for this category (if provided)
              if [ -f "remediation/${category}.auto.tfvars" ]; then
                cp "remediation/${category}.auto.tfvars" "$work/"
              fi
              if [ -f "remediation/terraform.auto.tfvars" ]; then
                cp "remediation/terraform.auto.tfvars" "$work/"
              fi

              # Check if any resource blocks remain
              if ! grep -rEq '^\s*resource\s+"' "$work"/*.tf 2>/dev/null; then
                echo "SKIP $category: no resource blocks after cleanup" | tee -a /tmp/tfplan.txt /tmp/tfgate.txt
                exitcode=1
                continue
              fi

              cat > "$work/backend.tf" <<EOF
          terraform {
            backend "s3" {
              bucket         = "${TF_BACKEND_BUCKET}"
              key            = "${state_key}"
              region         = "${TF_BACKEND_REGION}"
              dynamodb_table = "${TF_BACKEND_TABLE}"
              encrypt        = true
            }
          }
          EOF

              cat > "$work/provider.tf" <<'EOF'
          provider "aws" {
            region = "ap-northeast-2"
          }
          EOF

              cat > "$work/data.tf" <<'EOF'
          data "aws_caller_identity" "current" {}
          data "aws_region" "current" {}
          data "aws_partition" "current" {}
          EOF

              cat > "$work/.tflint.hcl" <<'EOF'
          rule "terraform_unused_declarations" {
            enabled = false
          }
          rule "terraform_required_providers" {
            enabled = false
          }
          rule "terraform_required_version" {
            enabled = false
          }
          EOF

              terraform -chdir="$work" init -input=false 2>&1 | tee -a /tmp/tfplan.txt
              if ! (cd "$work" && tflint -f compact 2>&1 | tee -a /tmp/tfgate.txt); then
                echo "SKIP $category: tflint failed" | tee -a /tmp/tfplan.txt /tmp/tfgate.txt
                exitcode=1
                continue
              fi
              # tfsec: record warnings only (non-blocking for remediation code)
              tfsec --no-colour "$work" 2>&1 | tee -a /tmp/tfgate.txt || echo "WARN $category: tfsec found issues (non-blocking)" | tee -a /tmp/tfplan.txt
              if ! terraform -chdir="$work" validate 2>&1 | tee -a /tmp/tfplan.txt /tmp/tfgate.txt; then
                echo "SKIP $category: validate failed" | tee -a /tmp/tfplan.txt /tmp/tfgate.txt
                exitcode=1
                continue
              fi
              terraform -chdir="$work" plan -no-color -out=tfplan.binary | tee -a /tmp/tfplan.txt
             
              terraform -chdir="$work" show -json tfplan.binary > "$work/plan.json"
              infracost breakdown --path "$work/plan.json" \
                --format json \
                --out-file "/tmp/infracost-${category}.json" 2>&1 | tee -a /tmp/tfplan.txt
                            if [ ${PIPESTATUS[0]} -ne 0 ]; then exitcode=1; fi
                          done
                        fi
                        
          # Merge all infracost outputs
          cost_files=""
          for f in /tmp/infracost-*.json; do
            [ -f "$f" ] && cost_files="$cost_files --path $f"
          done
          if [ -n "$cost_files" ]; then
            infracost output --format json $cost_files --out-file /tmp/infracost-total.json
            infracost output --format table $cost_files --out-file /tmp/infracost-table.txt
          fi

          # Build categorized cost summaries for richer PR comments.
          python3 - <<'PY'
          import glob
          import json
          import os
          import re
          from collections import defaultdict

          def to_float(v):
              try:
                  if v is None or v == "":
                      return 0.0
                  if isinstance(v, str):
                      v = v.replace(",", "").replace("$", "").strip()
                  return float(v)
              except Exception:
                  return 0.0

          def resource_monthly(resource):
              if not isinstance(resource, dict):
                  return 0.0
              direct = to_float(resource.get("monthlyCost"))
              if direct != 0.0:
                  return direct
              comps = resource.get("costComponents")
              if isinstance(comps, list):
                  return sum(to_float(c.get("monthlyCost")) for c in comps if isinstance(c, dict))
              return 0.0

          def extract_total(data):
              if not isinstance(data, dict):
                  return 0.0
              if "totalMonthlyCost" in data:
                  return to_float(data.get("totalMonthlyCost"))
              projects = data.get("projects")
              if isinstance(projects, list):
                  total = 0.0
                  for p in projects:
                      if not isinstance(p, dict):
                          continue
                      total += to_float((p.get("breakdown") or {}).get("totalMonthlyCost"))
                  if total > 0:
                      return total
                  for p in projects:
                      if not isinstance(p, dict):
                          continue
                      resources = (p.get("breakdown") or {}).get("resources") or []
                      if isinstance(resources, list):
                          total += sum(resource_monthly(r) for r in resources)
                  return total
              return 0.0

          def service_name(resource):
              if not isinstance(resource, dict):
                  return "unknown"
              rtype = str(resource.get("resourceType") or "")
              if rtype:
                  m = re.match(r"aws_([^_]+)", rtype)
                  return m.group(1) if m else rtype
              name = str(resource.get("name") or "")
              m = re.match(r"aws_([^_]+)", name)
              return m.group(1) if m else "unknown"

          def resource_label(resource):
              if not isinstance(resource, dict):
                  return "unknown"
              return str(resource.get("name") or resource.get("resourceType") or "unknown")

          def write_md(path, headers, rows):
              with open(path, "w", encoding="utf-8") as f:
                  f.write("| " + " | ".join(headers) + " |\n")
                  f.write("| " + " | ".join(["---"] * len(headers)) + " |\n")
                  for row in rows:
                      f.write("| " + " | ".join(row) + " |\n")

          category_totals = {}
          service_totals = defaultdict(float)
          resource_totals = defaultdict(float)

          for path in sorted(glob.glob("/tmp/infracost-*.json")):
              base = os.path.basename(path)
              if base == "infracost-total.json":
                  continue
              m = re.match(r"infracost-(.+)\.json$", base)
              if not m:
                  continue
              category = m.group(1)
              try:
                  with open(path, "r", encoding="utf-8") as f:
                      data = json.load(f)
              except Exception:
                  continue

              category_totals[category] = category_totals.get(category, 0.0) + extract_total(data)
              projects = data.get("projects") if isinstance(data, dict) else []
              if not isinstance(projects, list):
                  continue
              for p in projects:
                  if not isinstance(p, dict):
                      continue
                  resources = (p.get("breakdown") or {}).get("resources") or []
                  if not isinstance(resources, list):
                      continue
                  for r in resources:
                      monthly = resource_monthly(r)
                      if monthly <= 0:
                          continue
                      svc = service_name(r)
                      label = resource_label(r)
                      service_totals[svc] += monthly
                      resource_totals[f"{svc}:{label}"] += monthly

          category_rows = sorted(category_totals.items(), key=lambda x: x[1], reverse=True)
          service_rows = sorted(service_totals.items(), key=lambda x: x[1], reverse=True)
          resource_rows = sorted(resource_totals.items(), key=lambda x: x[1], reverse=True)[:15]

          write_md(
              "/tmp/infracost-category-summary.md",
              ["Category", "Monthly Cost (USD)"],
              [[k, f"${v:.2f}"] for k, v in category_rows] or [["(none)", "$0.00"]],
          )
          write_md(
              "/tmp/infracost-service-summary.md",
              ["Service", "Monthly Cost (USD)"],
              [[k, f"${v:.2f}"] for k, v in service_rows] or [["(none)", "$0.00"]],
          )
          write_md(
              "/tmp/infracost-top-resources.md",
              ["Resource", "Monthly Cost (USD)"],
              [[k.split(":", 1)[1], f"${v:.2f}"] for k, v in resource_rows] or [["(none)", "$0.00"]],
          )
          PY
          echo "exitcode=$exitcode" >> "$GITHUB_OUTPUT"

      - name: Post Plan to PR comment
        uses: actions/github-script@v7
        env:
          PLAN_OUTPUT: ${{ steps.plan.outputs.exitcode }}
        with:
          script: |
            const fs = require('fs');
            let plan = fs.readFileSync('/tmp/tfplan.txt', 'utf8');
            if (plan.length > 50000) {
              plan = plan.substring(0, 50000) + '\n\n... (truncated)';
            }

            // Read Infracost outputs
            const readIfExists = (path) => fs.existsSync(path) ? fs.readFileSync(path, 'utf8') : '';
            const toNumber = (value) => {
              if (value === null || value === undefined) return 0;
              const n = parseFloat(String(value).replace(/[$,]/g, '').trim());
              return Number.isFinite(n) ? n : 0;
            };
            const extractMonthlyCost = (costJson) => {
              if (!costJson || typeof costJson !== 'object') return 0;
              if (costJson.totalMonthlyCost !== undefined) return toNumber(costJson.totalMonthlyCost);
              if (Array.isArray(costJson.projects)) {
                return costJson.projects.reduce((sum, p) => sum + toNumber(p?.breakdown?.totalMonthlyCost), 0);
              }
              return 0;
            };

            let costSection = '';
            try {
              const costTable = readIfExists('/tmp/infracost-table.txt');
              const categorySummary = readIfExists('/tmp/infracost-category-summary.md');
              const serviceSummary = readIfExists('/tmp/infracost-service-summary.md');
              const topResources = readIfExists('/tmp/infracost-top-resources.md');
              const totalCostJson = readIfExists('/tmp/infracost-total.json');
              const costJson = totalCostJson ? JSON.parse(totalCostJson) : {};
              const monthly = extractMonthlyCost(costJson);
              const emoji = monthly > 100 ? 'HIGH' : monthly > 0 ? 'INFO' : 'OK';

              costSection = `\n### ${emoji} Estimated Monthly Cost: $${monthly.toFixed(2)}\n`;
              if (categorySummary) {
                costSection += `\n#### By Category\n${categorySummary}\n`;
              }
              if (serviceSummary) {
                costSection += `\n#### By Service\n${serviceSummary}\n`;
              }
              if (topResources) {
                costSection += `\n#### Top Resources (Monthly)\n${topResources}\n`;
              }
              if (costTable) {
                costSection += `\n<details>\n<summary>Infracost Raw Table (click to expand)</summary>\n\n\`\`\`\n${costTable}\n\`\`\`\n\n</details>\n`;
              }
            } catch (e) {
              costSection = '\n### Cost details unavailable (Infracost failed)\n';
            }

            const exitcode = process.env.PLAN_OUTPUT;
            const status = exitcode === '0' ? 'Plan Succeeded' : 'Plan Failed';

            const body = `### Terraform Plan Result - ${status}
            ${costSection}
            <details>
            <summary>Plan details (click to expand)</summary>

            \`\`\`
            ${plan}
            \`\`\`

            </details>

            > Merging this PR will auto-apply these resources to AWS.`;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Terraform Plan Result')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body,
              });
            }

      - name: Fail if plan failed
        if: steps.plan.outputs.exitcode != '0'
        run: exit 1

      - name: Upload quality gate logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: remediation-quality-gates
          path: /tmp/tfgate.txt
      
      - name: Upload cost estimate
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: remediation-cost-estimate
          path: |
            /tmp/infracost-*.json
            /tmp/infracost-*.md
            /tmp/infracost-table.txt

  # -------------------------------------------------------
  # Apply: run after PR merge and apply to AWS
  # -------------------------------------------------------
  apply:
    if: github.event.pull_request.merged == true
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ap-northeast-2

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0

      - name: Install IaC quality tools (tflint + tfsec)
        run: |
          curl -sSL -o /usr/local/bin/tfsec https://github.com/aquasecurity/tfsec/releases/latest/download/tfsec-linux-amd64
          chmod +x /usr/local/bin/tfsec
          curl -sSL -o /tmp/tflint.zip https://github.com/terraform-linters/tflint/releases/latest/download/tflint_linux_amd64.zip
          unzip -q /tmp/tflint.zip -d /tmp
          mv /tmp/tflint /usr/local/bin/tflint
          tflint --init

      - name: Install Prowler verification dependencies
        run: |
          pip install prowler pandas

      - name: Collect changed files and group by category
        id: changed
        run: |
          files=$(git diff --name-status ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }} -- 'remediation/*.tf' | awk '$1!="D"{print $NF}' | tr '\n' ' ')
          echo "files=$files" >> $GITHUB_OUTPUT

          categories=""
          for file in $files; do
            base=$(basename "$file" .tf | sed 's/^fix-//')
            cat=$(echo "$base" | sed 's/_.*//')
            if ! echo "$categories" | grep -qw "$cat"; then
              categories="$categories $cat"
            fi
          done
          echo "categories=$(echo $categories | xargs)" >> $GITHUB_OUTPUT

      - name: Collect target check IDs from manifest
        id: target_checks
        env:
          CATEGORIES: ${{ steps.changed.outputs.categories }}
        run: |
          python3 - <<'PY'
          import json
          import os

          categories = {c.strip() for c in os.environ.get("CATEGORIES", "").split() if c.strip()}
          check_ids = []
          if categories and os.path.exists("remediation/manifest.json"):
              with open("remediation/manifest.json", "r", encoding="utf-8") as f:
                  items = json.load(f)
              check_ids = sorted({
                  str(item.get("check_id", "")).strip()
                  for item in items
                  if str(item.get("category", "")).strip() in categories
                  and str(item.get("check_id", "")).strip()
              })

          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as out:
              out.write(f"check_ids={','.join(check_ids)}\n")

          print(f"Target categories: {sorted(categories)}")
          print(f"Target check_ids ({len(check_ids)}): {check_ids}")
          PY

      - name: Terraform Apply (per category)
        run: |
          set -euo pipefail
          : > /tmp/tfapply.txt
          applied=0
          skipped=0
          failed=0

          if [ -z "${{ steps.changed.outputs.categories }}" ]; then
            echo "No remediation categories changed."
            exit 0
          fi

          for category in ${{ steps.changed.outputs.categories }}; do
            echo "## Category: $category" >> /tmp/tfapply.txt
            work=$(mktemp -d)
            state_key="remediation/${category}.tfstate"

            # Copy only files listed in remediation/manifest.json for this category.
            # This prevents stale category files from being applied accidentally.
            file_count=0
            cat_files=$(CATEGORY="$category" python3 - <<'PY'
          import json
          import os

          category = os.environ.get("CATEGORY", "").strip()
          manifest = "remediation/manifest.json"
          selected = []
          if category and os.path.exists(manifest):
              with open(manifest, "r", encoding="utf-8") as f:
                  items = json.load(f)
              seen = set()
              for item in items:
                  if str(item.get("category", "")).strip() != category:
                      continue
                  fname = str(item.get("file", "")).strip()
                  if not fname or fname in seen:
                      continue
                  seen.add(fname)
                  selected.append(f"remediation/{fname}")
          print("\n".join(selected))
          PY
            )
            for f in $cat_files; do
              [ -f "$f" ] || continue
              cp "$f" "$work/"
              file_count=$((file_count+1))
            done

            if [ "$file_count" -eq 0 ]; then
              echo "SKIP $category: no files found" | tee -a /tmp/tfapply.txt
              skipped=$((skipped+1))
              continue
            fi

            echo "  Files: $file_count" >> /tmp/tfapply.txt

            # Run cleanup on each file
            for tf in "$work"/*.tf; do
              python3 iac/scripts/cleanup_hcl.py "$tf"
            done

            # Deduplicate resource/data blocks across files
            python3 iac/scripts/dedup_resources.py "$work"

            # Copy optional tfvars for this category (if provided)
            if [ -f "remediation/${category}.auto.tfvars" ]; then
              cp "remediation/${category}.auto.tfvars" "$work/"
            fi
            if [ -f "remediation/terraform.auto.tfvars" ]; then
              cp "remediation/terraform.auto.tfvars" "$work/"
            fi

            # Check if any resource blocks remain
            if ! grep -rEq '^\s*resource\s+"' "$work"/*.tf 2>/dev/null; then
              echo "SKIP $category: no resource blocks after cleanup" | tee -a /tmp/tfapply.txt
              skipped=$((skipped+1))
              failed=1
              continue
            fi

            cat > "$work/backend.tf" <<EOF
          terraform {
            backend "s3" {
              bucket         = "${TF_BACKEND_BUCKET}"
              key            = "${state_key}"
              region         = "${TF_BACKEND_REGION}"
              dynamodb_table = "${TF_BACKEND_TABLE}"
              encrypt        = true
            }
          }
          EOF

            cat > "$work/provider.tf" <<'EOF'
          provider "aws" {
            region = "ap-northeast-2"
          }
          EOF

            cat > "$work/data.tf" <<'EOF'
          data "aws_caller_identity" "current" {}
          data "aws_region" "current" {}
          data "aws_partition" "current" {}
          EOF

            cat > "$work/.tflint.hcl" <<'EOF'
          rule "terraform_unused_declarations" {
            enabled = false
          }
          rule "terraform_required_providers" {
            enabled = false
          }
          rule "terraform_required_version" {
            enabled = false
          }
          EOF

            terraform -chdir="$work" init -input=false 2>&1 | tee -a /tmp/tfapply.txt
            if ! (cd "$work" && tflint -f compact 2>&1 | tee -a /tmp/tfapply.txt); then
              echo "SKIP $category: tflint failed" | tee -a /tmp/tfapply.txt
              failed=1
              continue
            fi
            # tfsec: record warnings only (non-blocking for remediation code)
            tfsec --no-colour "$work" 2>&1 | tee -a /tmp/tfapply.txt || echo "WARN $category: tfsec found issues (non-blocking)" | tee -a /tmp/tfapply.txt
            if ! terraform -chdir="$work" validate 2>&1 | tee -a /tmp/tfapply.txt; then
              echo "SKIP $category: validate failed" | tee -a /tmp/tfapply.txt
              failed=1
              continue
            fi
            if ! terraform -chdir="$work" plan -out=tfplan 2>&1 | tee -a /tmp/tfapply.txt; then
              echo "SKIP $category: plan failed" | tee -a /tmp/tfapply.txt
              failed=1
              continue
            fi
            if ! terraform -chdir="$work" apply -auto-approve tfplan 2>&1 | tee -a /tmp/tfapply.txt; then
              echo "SKIP $category: apply failed" | tee -a /tmp/tfapply.txt
              failed=1
              continue
            fi
            applied=$((applied+1))
          done

          echo "applied=$applied skipped=$skipped failed=$failed" >> /tmp/tfapply.txt
          if [ "$applied" -eq 0 ]; then
            echo "No categories applied. Failing job." | tee -a /tmp/tfapply.txt
            exit 1
          fi
          if [ "$failed" -ne 0 ]; then
            echo "One or more categories failed. Failing job." | tee -a /tmp/tfapply.txt
            exit 1
          fi

      - name: Post-apply Prowler scan for target checks
        if: steps.target_checks.outputs.check_ids != ''
        env:
          CHECK_IDS: ${{ steps.target_checks.outputs.check_ids }}
        run: |
          set -euo pipefail
          mkdir -p /tmp/post_scan

          set +e
          prowler aws \
            --region ap-northeast-2 \
            --compliance cis_1.4_aws \
            --severity medium \
            --output-directory /tmp/post_scan \
            --output-formats csv
          prowler_exit=$?
          set -e

          # Prowler can return 3 when findings exist; that should not stop apply flow.
          if [ "$prowler_exit" -ne 0 ] && [ "$prowler_exit" -ne 3 ]; then
            echo "Unexpected prowler exit code: $prowler_exit"
            exit "$prowler_exit"
          fi

          python3 - <<'PY'
          import glob
          import json
          import os
          import pandas as pd

          check_ids = {x.strip() for x in os.environ.get("CHECK_IDS", "").split(",") if x.strip()}
          frames = []
          for path in glob.glob("/tmp/post_scan/*.csv"):
              try:
                  df = pd.read_csv(path, sep=";")
              except Exception:
                  continue
              if {"CHECK_ID", "STATUS"}.issubset(df.columns):
                  frames.append(df[["CHECK_ID", "STATUS"]])

          data = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=["CHECK_ID", "STATUS"])
          if check_ids:
              data = data[data["CHECK_ID"].astype(str).isin(check_ids)]

          fails = data[data["STATUS"].astype(str).str.upper() == "FAIL"]
          fail_count = int(len(fails))
          fail_by_check = fails.groupby("CHECK_ID").size().sort_values(ascending=False).to_dict()

          with open("/tmp/post_fail_count.txt", "w", encoding="utf-8") as f:
              f.write(str(fail_count))
          with open("/tmp/post_fail_by_check.json", "w", encoding="utf-8") as f:
              json.dump(fail_by_check, f, indent=2)

          print(f"Post-apply FAIL count (target checks): {fail_count}")
          PY

      - name: Trigger Prowler Re-scan
        run: |
          gh workflow run prowler-security-scan.yml
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Upload apply logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: remediation-apply-logs
          path: |
            /tmp/tfapply.txt
            /tmp/post_fail_count.txt
            /tmp/post_fail_by_check.json

